{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch= 0 err= 7698.62 accuracy= 0.01140625\n",
      "epoch= 0 err= 7744.3467 accuracy= 0.01390625\n",
      "epoch= 0 err= 7721.1577 accuracy= 0.0115625\n",
      "epoch= 0 err= 7759.4263 accuracy= 0.013125\n",
      "epoch= 1 err= 7728.9556 accuracy= 0.01203125\n",
      "epoch= 1 err= 7733.726 accuracy= 0.01484375\n",
      "epoch= 1 err= 7732.635 accuracy= 0.01125\n",
      "epoch= 1 err= 7719.696 accuracy= 0.01421875\n",
      "epoch= 2 err= 7727.689 accuracy= 0.011875\n",
      "epoch= 2 err= 7720.292 accuracy= 0.01265625\n",
      "epoch= 2 err= 7739.0312 accuracy= 0.0115625\n",
      "epoch= 2 err= 7751.301 accuracy= 0.01234375\n",
      "epoch= 3 err= 7729.0 accuracy= 0.0128125\n",
      "epoch= 3 err= 7706.5625 accuracy= 0.014375\n",
      "epoch= 3 err= 7721.0586 accuracy= 0.01109375\n",
      "epoch= 3 err= 7755.025 accuracy= 0.011875\n",
      "epoch= 4 err= 7710.2593 accuracy= 0.0115625\n",
      "epoch= 4 err= 7733.8496 accuracy= 0.01265625\n",
      "epoch= 4 err= 7680.9644 accuracy= 0.01203125\n",
      "epoch= 4 err= 7737.3555 accuracy= 0.01390625\n",
      "epoch= 5 err= 7730.105 accuracy= 0.013125\n",
      "epoch= 5 err= 7703.3125 accuracy= 0.01515625\n",
      "epoch= 5 err= 7737.3594 accuracy= 0.0109375\n",
      "epoch= 5 err= 7766.1504 accuracy= 0.0128125\n",
      "epoch= 6 err= 7747.7676 accuracy= 0.01265625\n",
      "epoch= 6 err= 7720.886 accuracy= 0.01375\n",
      "epoch= 6 err= 7713.8765 accuracy= 0.01125\n",
      "epoch= 6 err= 7723.6265 accuracy= 0.01171875\n",
      "epoch= 7 err= 7750.822 accuracy= 0.01171875\n",
      "epoch= 7 err= 7732.6636 accuracy= 0.01296875\n",
      "epoch= 7 err= 7729.5244 accuracy= 0.0121875\n",
      "epoch= 7 err= 7744.1724 accuracy= 0.01390625\n",
      "epoch= 8 err= 7720.578 accuracy= 0.01296875\n",
      "epoch= 8 err= 7741.92 accuracy= 0.01453125\n",
      "epoch= 8 err= 7723.3633 accuracy= 0.01140625\n",
      "epoch= 8 err= 7773.7495 accuracy= 0.0115625\n",
      "epoch= 9 err= 7717.3926 accuracy= 0.01234375\n",
      "epoch= 9 err= 7724.666 accuracy= 0.01390625\n",
      "epoch= 9 err= 7729.2676 accuracy= 0.0115625\n",
      "epoch= 9 err= 7730.198 accuracy= 0.01265625\n",
      "epoch= 10 err= 7721.364 accuracy= 0.01203125\n",
      "epoch= 10 err= 7688.3213 accuracy= 0.01484375\n",
      "epoch= 10 err= 7726.8535 accuracy= 0.0096875\n",
      "epoch= 10 err= 7720.2725 accuracy= 0.01265625\n",
      "epoch= 11 err= 7728.6377 accuracy= 0.01296875\n",
      "epoch= 11 err= 7754.8813 accuracy= 0.013125\n",
      "epoch= 11 err= 7731.98 accuracy= 0.0090625\n",
      "epoch= 11 err= 7733.973 accuracy= 0.01375\n",
      "epoch= 12 err= 7724.228 accuracy= 0.01234375\n",
      "epoch= 12 err= 7710.99 accuracy= 0.014375\n",
      "epoch= 12 err= 7716.3975 accuracy= 0.010625\n",
      "epoch= 12 err= 7753.339 accuracy= 0.0128125\n",
      "epoch= 13 err= 7735.8677 accuracy= 0.01203125\n",
      "epoch= 13 err= 7726.1294 accuracy= 0.01453125\n",
      "epoch= 13 err= 7730.435 accuracy= 0.01109375\n",
      "epoch= 13 err= 7744.5254 accuracy= 0.01328125\n",
      "epoch= 14 err= 7759.4956 accuracy= 0.0121875\n",
      "epoch= 14 err= 7713.275 accuracy= 0.01484375\n",
      "epoch= 14 err= 7729.902 accuracy= 0.01140625\n",
      "epoch= 14 err= 7733.4473 accuracy= 0.01375\n",
      "epoch= 15 err= 7727.6714 accuracy= 0.011875\n",
      "epoch= 15 err= 7733.51 accuracy= 0.0125\n",
      "epoch= 15 err= 7710.797 accuracy= 0.0125\n",
      "epoch= 15 err= 7766.8687 accuracy= 0.01421875\n",
      "epoch= 16 err= 7714.7476 accuracy= 0.01265625\n",
      "epoch= 16 err= 7746.0464 accuracy= 0.0134375\n",
      "epoch= 16 err= 7729.34 accuracy= 0.011875\n",
      "epoch= 16 err= 7731.1914 accuracy= 0.01359375\n",
      "epoch= 17 err= 7717.132 accuracy= 0.01296875\n",
      "epoch= 17 err= 7713.4336 accuracy= 0.01390625\n",
      "epoch= 17 err= 7688.303 accuracy= 0.0109375\n",
      "epoch= 17 err= 7731.665 accuracy= 0.01390625\n",
      "epoch= 18 err= 7724.6514 accuracy= 0.01234375\n",
      "epoch= 18 err= 7734.0713 accuracy= 0.014375\n",
      "epoch= 18 err= 7748.5273 accuracy= 0.01046875\n",
      "epoch= 18 err= 7747.2427 accuracy= 0.013125\n",
      "epoch= 19 err= 7754.537 accuracy= 0.0115625\n",
      "epoch= 19 err= 7717.2095 accuracy= 0.01421875\n",
      "epoch= 19 err= 7717.346 accuracy= 0.010625\n",
      "epoch= 19 err= 7737.7285 accuracy= 0.01296875\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from os.path import isfile, join\n",
    "from os import listdir\n",
    "from random import shuffle\n",
    "from shutil import copyfile\n",
    "import os\n",
    "import tensorflow as tf \n",
    "import datetime\n",
    "\n",
    "\n",
    "\n",
    "class DataSetGenerator:\n",
    "    def __init__(self, data_dir):\n",
    "        self.data_dir = data_dir\n",
    "        self.data_labels = self.get_data_labels()\n",
    "        self.data_info = self.get_data_paths()\n",
    "\n",
    "    def get_data_labels(self):\n",
    "        data_labels = []\n",
    "        for filename in listdir(self.data_dir):\n",
    "            if not isfile(join(self.data_dir, filename)):\n",
    "                data_labels.append(filename)\n",
    "        return data_labels\n",
    "\n",
    "    def get_data_paths(self):\n",
    "        data_paths = []\n",
    "        for label in self.data_labels:\n",
    "            img_lists=[]\n",
    "            path = join(self.data_dir, label)\n",
    "            for filename in listdir(path):\n",
    "                tokens = filename.split('.')\n",
    "                if tokens[-1] == 'jpg':\n",
    "                    image_path=join(path, filename)\n",
    "                    img_lists.append(image_path)\n",
    "            shuffle(img_lists)\n",
    "            data_paths.append(img_lists)\n",
    "        return data_paths\n",
    "    def get_mini_batches(self, batch_size=6400, image_size=(100, 100), allchannel=True):\n",
    "        images = []\n",
    "        labels = []\n",
    "        empty=False\n",
    "        counter=0\n",
    "        each_batch_size=int(batch_size/len(self.data_info))\n",
    "        while True:\n",
    "            for i in range(len(self.data_labels)):\n",
    "                label = np.zeros(len(self.data_labels),dtype=int)\n",
    "                label[i] = 1\n",
    "                if len(self.data_info[i]) < counter+1:\n",
    "                    empty=True\n",
    "                    continue\n",
    "                empty=False\n",
    "                img = cv2.imread(self.data_info[i][counter])\n",
    "                if not allchannel:\n",
    "                    img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "                    img = np.reshape(img, (img.shape[0], img.shape[1], 1))\n",
    "                images.append(img)\n",
    "                labels.append(label)\n",
    "            counter+=1\n",
    "            if empty:\n",
    "                break\n",
    "            # if the iterator is multiple of batch size return the mini batch\n",
    "            if (counter)%each_batch_size == 0:\n",
    "                yield np.array(images,dtype=np.uint8), np.array(labels,dtype=np.uint8)\n",
    "                del images\n",
    "                del labels\n",
    "                images=[]\n",
    "                labels=[]\n",
    "    \n",
    "    \n",
    "    \n",
    "class NetworkBuilder: \n",
    "    def __init__(self): \n",
    "        pass\n",
    "    def attach_conv_layer(self, input_layer, output_size, feature_size, strides, padding='SAME',\n",
    "                          summary=False):\n",
    "        with tf.name_scope(\"Convolution\") as scope:\n",
    "            input_size = input_layer.get_shape().as_list()[-1]\n",
    "            weights = tf.Variable(tf.random_normal([feature_size[0], feature_size[1], input_size, output_size]), name='conv_weights')\n",
    "            if summary:\n",
    "                tf.summary.histogram(weights.name, weights)\n",
    "            biases = tf.Variable(tf.random_normal([output_size]),name='conv_biases')\n",
    "            conv = tf.nn.conv2d(input_layer, weights, strides=strides, padding=padding)+biases\n",
    "            return conv\n",
    "    \n",
    "    def attach_pooling_layer(self, input_layer, ksize, strides, padding='SAME'):\n",
    "        with tf.name_scope(\"Pooling\") as scope:\n",
    "            return tf.nn.max_pool(input_layer, ksize=ksize, strides=strides, padding=padding)\n",
    "        \n",
    "        \n",
    "    def flatten(self, input_layer):\n",
    "        with tf.name_scope(\"Flatten\") as scope:\n",
    "            input_size = input_layer.get_shape().as_list()\n",
    "            new_size = input_size[-1]*input_size[-2]*input_size[-3]\n",
    "            return tf.reshape(input_layer, [-1, new_size])\n",
    "\n",
    "    \n",
    "    def attach_relu_layer(self, input_layer):\n",
    "        with tf.name_scope(\"Activation\") as scope:\n",
    "            return tf.nn.relu(input_layer)\n",
    "\n",
    "nb=NetworkBuilder()\n",
    "    \n",
    "input_img = tf.placeholder(tf.float32, shape=[None, 100, 100, 1], name=\"input\")\n",
    "target_labels = tf.placeholder(tf.int64, shape=[None,64], name=\"Targets\")\n",
    "\n",
    "with tf.name_scope(\"ModelV2\") as scope:\n",
    "    \n",
    "    model = input_img\n",
    "    model1 = nb.attach_conv_layer( model, output_size=4, feature_size=(5,5), strides=[1,2,2,1], padding='SAME',\n",
    "                          summary=False)\n",
    "    model1 = nb.attach_pooling_layer(input_layer=model1, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')\n",
    "\n",
    "    model2 = nb.attach_conv_layer( model1, output_size=16, feature_size=(5,5), strides=[1,2,2,1], padding='SAME',\n",
    "                          summary=False)\n",
    "    model2 = nb.attach_pooling_layer(input_layer=model2, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')\n",
    "\n",
    "    model3 = nb.attach_conv_layer( model2, output_size=32, feature_size=(5,5), strides=[1,2,2,1], padding='SAME',\n",
    "                          summary=False)\n",
    "    model3 = nb.attach_pooling_layer(input_layer=model3, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')\n",
    "\n",
    "    model4 = nb.attach_conv_layer( model3, output_size=64, feature_size=(5,5), strides=[1,2,2,1], padding='SAME',\n",
    "                          summary=False)\n",
    "    model4 = nb.attach_pooling_layer(input_layer=model4, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')\n",
    "    fc1 = nb.flatten(model4)\n",
    "    fc2 = nb.attach_relu_layer(fc1)\n",
    "    fc3  = tf.nn.dropout(fc2,keep_prob=0.8)\n",
    "    prediction = tf.nn.softmax(fc3)\n",
    "    \n",
    "epochs = 20\n",
    "batchSize = 6400\n",
    "\n",
    "\n",
    "with tf.name_scope(\"Optimization\") as scope:\n",
    "    global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "    cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=fc3, labels=target_labels)\n",
    "    cost = tf.reduce_mean(cross_entropy)\n",
    "    tf.summary.scalar(\"cost\", cost)\n",
    "\n",
    "with tf.name_scope('accuracy') as scope:\n",
    "    correct_pred = tf.equal(tf.argmax(prediction, 1), tf.argmax(target_labels, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "dg = DataSetGenerator(\"./fruit/Training\")  \n",
    "\n",
    "with tf.Session() as sess:\n",
    "    tf.global_variables_initializer().run()\n",
    "    for epoch in range(epochs):\n",
    "        batches = dg.get_mini_batches(batchSize,(100,100), allchannel=False)\n",
    "        for imgs ,labels in batches:\n",
    "            imgs=np.divide(imgs, 255)\n",
    "            error, acu = sess.run([cost, accuracy],\n",
    "                                            feed_dict={input_img: imgs, target_labels: labels})\n",
    "            print(\"epoch=\", epoch, \"err=\", error, \"accuracy=\", acu)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
