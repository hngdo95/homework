{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import os\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import pandas as pd\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report\n",
    "import os\n",
    "import urllib.request   \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import LSTM,Flatten\n",
    "import numpy as np\n",
    "import gensim\n",
    "from gensim import corpora, models, similarities\n",
    "from gensim.parsing.preprocessing import remove_stopwords    \n",
    "from string import punctuation\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from sklearn import tree\n",
    "from keras.preprocessing.sequence import TimeseriesGenerator\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('data/Alexandre Dumas/THE_THREE_MUSKETEERS.txt',\n",
       " <http.client.HTTPMessage at 0x18a7dc06a20>)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if not os.path.exists('data'):\n",
    "    os.mkdir('data')\n",
    "\n",
    "urllib.request.urlretrieve('https://www.gutenberg.org/files/2591/2591-0.txt', 'data/Jacob and Wilhelm/grimms_fairy_tales.txt')\n",
    "urllib.request.urlretrieve('http://www.gutenberg.org/cache/epub/345/pg345.txt', 'data/Bram Stoker/dracula.txt')\n",
    "urllib.request.urlretrieve('http://www.gutenberg.org/cache/epub/84/pg84.txt', 'data/Mary Shelley/frankenstein.txt')\n",
    "urllib.request.urlretrieve('http://www.gutenberg.org/files/2701/2701-0.txt', 'data/Herman Melville/moby_dick.txt')\n",
    "urllib.request.urlretrieve('http://www.gutenberg.org/files/74/74-0.txt', 'data/Mark Twain/tom_sawyer.txt')\n",
    "urllib.request.urlretrieve('http://www.gutenberg.org/cache/epub/38189/pg38189.txt', 'data/Ambrose Blacklock/A_TREATISE_ON_SHEEP.txt')\n",
    "urllib.request.urlretrieve('http://www.gutenberg.org/cache/epub/2166/pg2166.txt', 'data/H.Rider Haggard/King_Solomons_Mines.txt')\n",
    "urllib.request.urlretrieve('http://www.gutenberg.org/cache/epub/21728/pg21728.txt', 'data/R.M.Ballantyne/THE_DOG_CRUSOE_AND_HIS_MASTER.txt')\n",
    "urllib.request.urlretrieve('https://www.gutenberg.org/files/95/95-0.txt', 'data/Anthony Hope/THE_PRISONER_OF_ZENDA.txt')\n",
    "urllib.request.urlretrieve('https://www.gutenberg.org/files/1257/1257-0.txt', 'data/Alexandre Dumas/THE_THREE_MUSKETEERS.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fname= ./data\\Alexandre Dumas\\THE_THREE_MUSKETEERS.txt\n",
      "fname= ./data\\Ambrose Blacklock\\A_TREATISE_ON_SHEEP.txt\n",
      "fname= ./data\\Anthony Hope\\THE_PRISONER_OF_ZENDA.txt\n",
      "fname= ./data\\Bram Stoker\\dracula.txt\n",
      "fname= ./data\\H.Rider Haggard\\King_Solomons_Mines.txt\n",
      "fname= ./data\\Herman Melville\\moby_dick.txt\n",
      "fname= ./data\\Jacob and Wilhelm\\grimms_fairy_tales.txt\n",
      "fname= ./data\\Mark Twain\\tom_sawyer.txt\n",
      "fname= ./data\\Mary Shelley\\frankenstein.txt\n",
      "fname= ./data\\R.M.Ballantyne\\THE_DOG_CRUSOE_AND_HIS_MASTER.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3\\envs\\tensorflow1\\lib\\site-packages\\ipykernel_launcher.py:17: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "path = './data'\n",
    "token_dict = {}\n",
    "dfs=[]\n",
    "\n",
    "\n",
    "\n",
    "for dirpath, dirs, files in os.walk(path):\n",
    "    for f in files:\n",
    "        fname = os.path.join(dirpath, f)\n",
    "        fdir=os.path.basename(dirpath)\n",
    "        print (\"fname=\", fname)\n",
    "        dt=pd.read_fwf(fname, sep=\".\", header=None)\n",
    "        dt['label']=fdir\n",
    "        dfs.append(dt)\n",
    "\n",
    "            \n",
    "big_frame = pd.concat(dfs, ignore_index=True)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>label</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ï»¿                          THE THREE MUSKETEERS</td>\n",
       "      <td>Alexandre Dumas</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>This ebook is for the use of anyone anywhere i...</td>\n",
       "      <td>Alexandre Dumas</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>most other parts of the world at no cost and w...</td>\n",
       "      <td>Alexandre Dumas</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>whatsoever. You may copy it, give it away or r...</td>\n",
       "      <td>Alexandre Dumas</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>of the Project Gutenberg License included with...</td>\n",
       "      <td>Alexandre Dumas</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0            label    1\n",
       "0  ï»¿                          THE THREE MUSKETEERS  Alexandre Dumas  NaN\n",
       "1  This ebook is for the use of anyone anywhere i...  Alexandre Dumas  NaN\n",
       "2  most other parts of the world at no cost and w...  Alexandre Dumas  NaN\n",
       "3  whatsoever. You may copy it, give it away or r...  Alexandre Dumas  NaN\n",
       "4  of the Project Gutenberg License included with...  Alexandre Dumas  NaN"
      ]
     },
     "execution_count": 289,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "big_frame.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Alexandre Dumas      22602\n",
       "Herman Melville      19222\n",
       "Bram Stoker          13414\n",
       "Jacob and Wilhelm     9571\n",
       "R.M.Ballantyne        7830\n",
       "H.Rider Haggard       7451\n",
       "Mark Twain            6927\n",
       "Ambrose Blacklock     6577\n",
       "Anthony Hope          5206\n",
       "Mary Shelley          1300\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "big_frame[\"label\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "big_frame.dtypes\n",
    "big_frame[0] = big_frame[0].replace('\"', ' ', regex=True)\n",
    "big_frame[0] = big_frame[0].replace('!', ' ', regex=True)\n",
    "big_frame[0] = big_frame[0].replace('-', ' ', regex=True)\n",
    "big_frame[0] = big_frame[0].replace(',', ' ', regex=True)\n",
    "big_frame[0] = big_frame[0].replace('_', ' ', regex=True)\n",
    "big_frame[0] = big_frame[0].replace(';', ' ', regex=True)\n",
    "big_frame[0] = big_frame[0].replace('»', ' ', regex=True)\n",
    "big_frame[0] = big_frame[0].replace('@', ' ', regex=True)\n",
    "big_frame[0] = big_frame[0].replace('$', ' ', regex=True)\n",
    "big_frame[0] = big_frame[0].replace('&', ' ', regex=True)\n",
    "big_frame[0] = big_frame[0].replace('%', ' ', regex=True)\n",
    "big_frame[0] = big_frame[0].replace('â€™', ' ', regex=True)\n",
    "big_frame[0] = big_frame[0].replace('â€�', ' ', regex=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_x=big_frame[0].values.astype('U')\n",
    "df_y=big_frame[\"label\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Word tokenize+TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "stop_words = stopwords.words('english') + list(punctuation)\n",
    " \n",
    "def tokenize(text):\n",
    "    words = word_tokenize(text)\n",
    "    words = [w.lower() for w in words]\n",
    "    return [w for w in words if w not in stop_words and not w.isdigit()]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Naive Bayes classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cv = TfidfVectorizer(tokenizer=tokenize,min_df=1,ngram_range =(1,2))\n",
    "x_trainc=cv.fit_transform(df_x)\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_trainc, df_y, test_size=0.1, random_state=4)\n",
    "mnb = MultinomialNB(alpha=0.1)\n",
    "clf=mnb.fit(x_train,y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = clf.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   precision    recall  f1-score   support\n",
      "\n",
      "  Alexandre Dumas       0.66      0.88      0.75      2266\n",
      "Ambrose Blacklock       0.74      0.66      0.70       638\n",
      "     Anthony Hope       0.86      0.30      0.45       515\n",
      "      Bram Stoker       0.60      0.69      0.64      1348\n",
      "  H.Rider Haggard       0.74      0.41      0.53       778\n",
      "  Herman Melville       0.70      0.81      0.75      1907\n",
      "Jacob and Wilhelm       0.78      0.75      0.77       950\n",
      "       Mark Twain       0.85      0.50      0.63       665\n",
      "     Mary Shelley       1.00      0.84      0.92       135\n",
      "   R.M.Ballantyne       0.76      0.57      0.65       808\n",
      "\n",
      "      avg / total       0.72      0.70      0.69     10010\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Stochastic Gradient Descent classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3\\envs\\tensorflow1\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "svm=SGDClassifier()\n",
    "svm_classifier=svm.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_predicted = svm_classifier.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   precision    recall  f1-score   support\n",
      "\n",
      "  Alexandre Dumas       0.68      0.85      0.75      2266\n",
      "Ambrose Blacklock       0.68      0.70      0.69       638\n",
      "     Anthony Hope       0.77      0.36      0.49       515\n",
      "      Bram Stoker       0.64      0.61      0.63      1348\n",
      "  H.Rider Haggard       0.70      0.40      0.51       778\n",
      "  Herman Melville       0.65      0.79      0.71      1907\n",
      "Jacob and Wilhelm       0.70      0.75      0.73       950\n",
      "       Mark Twain       0.68      0.48      0.57       665\n",
      "     Mary Shelley       0.97      0.79      0.87       135\n",
      "   R.M.Ballantyne       0.71      0.53      0.60       808\n",
      "\n",
      "      avg / total       0.68      0.68      0.67     10010\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, svm_predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Decision Tree and Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x_trainc, df_y, test_size=0.2, random_state=4)\n",
    "Tree_clf = tree.DecisionTreeClassifier()\n",
    "Tree_clf = Tree_clf.fit(x_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   precision    recall  f1-score   support\n",
      "\n",
      "  Alexandre Dumas       0.60      0.65      0.62      4513\n",
      "Ambrose Blacklock       0.44      0.49      0.46      1294\n",
      "     Anthony Hope       0.43      0.30      0.36      1023\n",
      "      Bram Stoker       0.33      0.43      0.37      2677\n",
      "  H.Rider Haggard       0.38      0.29      0.33      1489\n",
      "  Herman Melville       0.57      0.60      0.59      3846\n",
      "Jacob and Wilhelm       0.55      0.50      0.52      1935\n",
      "       Mark Twain       0.47      0.36      0.41      1418\n",
      "     Mary Shelley       1.00      0.78      0.87       272\n",
      "   R.M.Ballantyne       0.50      0.40      0.44      1553\n",
      "\n",
      "      avg / total       0.51      0.50      0.50     20020\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predicted = Tree_clf.predict(x_test)\n",
    "print(classification_report(y_test, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "RF_clf = RandomForestClassifier(\n",
    "n_estimators=10,max_depth=40, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   precision    recall  f1-score   support\n",
      "\n",
      "  Alexandre Dumas       0.60      0.65      0.62      4513\n",
      "Ambrose Blacklock       0.44      0.49      0.46      1294\n",
      "     Anthony Hope       0.43      0.30      0.36      1023\n",
      "      Bram Stoker       0.33      0.43      0.37      2677\n",
      "  H.Rider Haggard       0.38      0.29      0.33      1489\n",
      "  Herman Melville       0.57      0.60      0.59      3846\n",
      "Jacob and Wilhelm       0.55      0.50      0.52      1935\n",
      "       Mark Twain       0.47      0.36      0.41      1418\n",
      "     Mary Shelley       1.00      0.78      0.87       272\n",
      "   R.M.Ballantyne       0.50      0.40      0.44      1553\n",
      "\n",
      "      avg / total       0.51      0.50      0.50     20020\n",
      "\n"
     ]
    }
   ],
   "source": [
    "RF_clf=RF_clf.fit(x_train,y_train)\n",
    "predicted = Tree_clf.predict(x_test)\n",
    "print(classification_report(y_test, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "big_frame = pd.get_dummies(big_frame, columns=['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>label_Alexandre Dumas</th>\n",
       "      <th>label_Ambrose Blacklock</th>\n",
       "      <th>label_Anthony Hope</th>\n",
       "      <th>label_Bram Stoker</th>\n",
       "      <th>label_H.Rider Haggard</th>\n",
       "      <th>label_Herman Melville</th>\n",
       "      <th>label_Jacob and Wilhelm</th>\n",
       "      <th>label_Mark Twain</th>\n",
       "      <th>label_Mary Shelley</th>\n",
       "      <th>label_R.M.Ballantyne</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ï ¿                          THE THREE MUSKETE...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>This ebook is for the use of anyone anywhere i...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>most other parts of the world at no cost and w...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>whatsoever. You may copy it  give it away or r...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>of the Project Gutenberg License included with...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>http://www.gutenberg.org/license. If you are n...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>States  you ll have to check the laws of the c...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>located before using this ebook.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Title: The Three Musketeers</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Author: Alexandre Dumas  Pere</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Release Date: March 01  1998 [EBook #1257]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Reposted: November 27  2016 [corrections made]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Language: English</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Character set encoding: UTF 8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>*** START OF THIS PROJECT GUTENBERG EBOOK THE ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>ALEXANDRE DUMAS  PERE ***</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Produced by John P. Roberts III  Roger Labbe  ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Asscher  Anita Martin  David Muller and David ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>*THE THREE MUSKETEERS*</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>By</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>*Alexandre Dumas  Pere*</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>First Volume of the d Artagnan Series</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>CONTENTS</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>AUTHOR S PREFACE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1 THE THREE PRESENTS OF D ARTAGNAN THE ELDER</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2 THE ANTECHAMBER OF M. DE TREVILLE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>3 THE AUDIENCE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>4 THE SHOULDER OF ATHOS  THE BALDRIC OF PORTHO...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>OF ARAMIS</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>5 THE KING S MUSKETEERS AND THE CARDINAL S GUA...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100070</th>\n",
       "      <td>where we have not received written confirmatio...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100071</th>\n",
       "      <td>SEND DONATIONS or determine the status of comp...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100072</th>\n",
       "      <td>particular state visit http://pglaf.org</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100073</th>\n",
       "      <td>While we cannot and do not solicit contributio...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100074</th>\n",
       "      <td>have not met the solicitation requirements  we...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100075</th>\n",
       "      <td>against accepting unsolicited donations from d...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100076</th>\n",
       "      <td>approach us with offers to donate.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100077</th>\n",
       "      <td>International donations are gratefully accepte...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100078</th>\n",
       "      <td>any statements concerning tax treatment of don...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100079</th>\n",
       "      <td>outside the United States.  U.S. laws alone sw...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100080</th>\n",
       "      <td>Please check the Project Gutenberg Web pages f...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100081</th>\n",
       "      <td>methods and addresses.  Donations are accepted...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100082</th>\n",
       "      <td>ways including checks  online payments and cre...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100083</th>\n",
       "      <td>To donate  please visit: http://pglaf.org/donate</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100084</th>\n",
       "      <td>Section 5.  General Information About Project ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100085</th>\n",
       "      <td>works.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100086</th>\n",
       "      <td>Professor Michael S. Hart is the originator of...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100087</th>\n",
       "      <td>concept of a library of electronic works that ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100088</th>\n",
       "      <td>with anyone.  For thirty years  he produced an...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100089</th>\n",
       "      <td>Gutenberg tm eBooks with only a loose network ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100090</th>\n",
       "      <td>Project Gutenberg tm eBooks are often created ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100091</th>\n",
       "      <td>editions  all of which are confirmed as Public...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100092</th>\n",
       "      <td>unless a copyright notice is included.  Thus  ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100093</th>\n",
       "      <td>keep eBooks in compliance with any particular ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100094</th>\n",
       "      <td>Most people start at our Web site which has th...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100095</th>\n",
       "      <td>http://www.gutenberg.org</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100096</th>\n",
       "      <td>This Web site includes information about Proje...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100097</th>\n",
       "      <td>including how to make donations to the Project...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100098</th>\n",
       "      <td>Archive Foundation  how to help produce our ne...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100099</th>\n",
       "      <td>subscribe to our email newsletter to hear abou...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100100 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                        0    1  \\\n",
       "0       ï ¿                          THE THREE MUSKETE...  NaN   \n",
       "1       This ebook is for the use of anyone anywhere i...  NaN   \n",
       "2       most other parts of the world at no cost and w...  NaN   \n",
       "3       whatsoever. You may copy it  give it away or r...  NaN   \n",
       "4       of the Project Gutenberg License included with...  NaN   \n",
       "5       http://www.gutenberg.org/license. If you are n...  NaN   \n",
       "6       States  you ll have to check the laws of the c...  NaN   \n",
       "7                       located before using this ebook.   NaN   \n",
       "8                            Title: The Three Musketeers   NaN   \n",
       "9                          Author: Alexandre Dumas  Pere   NaN   \n",
       "10            Release Date: March 01  1998 [EBook #1257]   NaN   \n",
       "11        Reposted: November 27  2016 [corrections made]   NaN   \n",
       "12                                     Language: English   NaN   \n",
       "13                         Character set encoding: UTF 8   NaN   \n",
       "14      *** START OF THIS PROJECT GUTENBERG EBOOK THE ...  NaN   \n",
       "15                             ALEXANDRE DUMAS  PERE ***   NaN   \n",
       "16      Produced by John P. Roberts III  Roger Labbe  ...  NaN   \n",
       "17      Asscher  Anita Martin  David Muller and David ...  NaN   \n",
       "18                                *THE THREE MUSKETEERS*   NaN   \n",
       "19                                                   By    NaN   \n",
       "20                               *Alexandre Dumas  Pere*   NaN   \n",
       "21                First Volume of the d Artagnan Series    NaN   \n",
       "22                                              CONTENTS   NaN   \n",
       "23                                      AUTHOR S PREFACE   NaN   \n",
       "24          1 THE THREE PRESENTS OF D ARTAGNAN THE ELDER   NaN   \n",
       "25                   2 THE ANTECHAMBER OF M. DE TREVILLE   NaN   \n",
       "26                                        3 THE AUDIENCE   NaN   \n",
       "27      4 THE SHOULDER OF ATHOS  THE BALDRIC OF PORTHO...  NaN   \n",
       "28                                             OF ARAMIS   NaN   \n",
       "29      5 THE KING S MUSKETEERS AND THE CARDINAL S GUA...  NaN   \n",
       "...                                                   ...  ...   \n",
       "100070  where we have not received written confirmatio...  NaN   \n",
       "100071  SEND DONATIONS or determine the status of comp...  NaN   \n",
       "100072           particular state visit http://pglaf.org   NaN   \n",
       "100073  While we cannot and do not solicit contributio...  NaN   \n",
       "100074  have not met the solicitation requirements  we...  NaN   \n",
       "100075  against accepting unsolicited donations from d...  NaN   \n",
       "100076                approach us with offers to donate.   NaN   \n",
       "100077  International donations are gratefully accepte...  NaN   \n",
       "100078  any statements concerning tax treatment of don...  NaN   \n",
       "100079  outside the United States.  U.S. laws alone sw...  NaN   \n",
       "100080  Please check the Project Gutenberg Web pages f...  NaN   \n",
       "100081  methods and addresses.  Donations are accepted...  NaN   \n",
       "100082  ways including checks  online payments and cre...  NaN   \n",
       "100083  To donate  please visit: http://pglaf.org/donate   NaN   \n",
       "100084  Section 5.  General Information About Project ...  NaN   \n",
       "100085                                            works.   NaN   \n",
       "100086  Professor Michael S. Hart is the originator of...  NaN   \n",
       "100087  concept of a library of electronic works that ...  NaN   \n",
       "100088  with anyone.  For thirty years  he produced an...  NaN   \n",
       "100089  Gutenberg tm eBooks with only a loose network ...  NaN   \n",
       "100090  Project Gutenberg tm eBooks are often created ...  NaN   \n",
       "100091  editions  all of which are confirmed as Public...  NaN   \n",
       "100092  unless a copyright notice is included.  Thus  ...  NaN   \n",
       "100093  keep eBooks in compliance with any particular ...  NaN   \n",
       "100094  Most people start at our Web site which has th...  NaN   \n",
       "100095                          http://www.gutenberg.org   NaN   \n",
       "100096  This Web site includes information about Proje...  NaN   \n",
       "100097  including how to make donations to the Project...  NaN   \n",
       "100098  Archive Foundation  how to help produce our ne...  NaN   \n",
       "100099  subscribe to our email newsletter to hear abou...  NaN   \n",
       "\n",
       "        label_Alexandre Dumas  label_Ambrose Blacklock  label_Anthony Hope  \\\n",
       "0                           1                        0                   0   \n",
       "1                           1                        0                   0   \n",
       "2                           1                        0                   0   \n",
       "3                           1                        0                   0   \n",
       "4                           1                        0                   0   \n",
       "5                           1                        0                   0   \n",
       "6                           1                        0                   0   \n",
       "7                           1                        0                   0   \n",
       "8                           1                        0                   0   \n",
       "9                           1                        0                   0   \n",
       "10                          1                        0                   0   \n",
       "11                          1                        0                   0   \n",
       "12                          1                        0                   0   \n",
       "13                          1                        0                   0   \n",
       "14                          1                        0                   0   \n",
       "15                          1                        0                   0   \n",
       "16                          1                        0                   0   \n",
       "17                          1                        0                   0   \n",
       "18                          1                        0                   0   \n",
       "19                          1                        0                   0   \n",
       "20                          1                        0                   0   \n",
       "21                          1                        0                   0   \n",
       "22                          1                        0                   0   \n",
       "23                          1                        0                   0   \n",
       "24                          1                        0                   0   \n",
       "25                          1                        0                   0   \n",
       "26                          1                        0                   0   \n",
       "27                          1                        0                   0   \n",
       "28                          1                        0                   0   \n",
       "29                          1                        0                   0   \n",
       "...                       ...                      ...                 ...   \n",
       "100070                      0                        0                   0   \n",
       "100071                      0                        0                   0   \n",
       "100072                      0                        0                   0   \n",
       "100073                      0                        0                   0   \n",
       "100074                      0                        0                   0   \n",
       "100075                      0                        0                   0   \n",
       "100076                      0                        0                   0   \n",
       "100077                      0                        0                   0   \n",
       "100078                      0                        0                   0   \n",
       "100079                      0                        0                   0   \n",
       "100080                      0                        0                   0   \n",
       "100081                      0                        0                   0   \n",
       "100082                      0                        0                   0   \n",
       "100083                      0                        0                   0   \n",
       "100084                      0                        0                   0   \n",
       "100085                      0                        0                   0   \n",
       "100086                      0                        0                   0   \n",
       "100087                      0                        0                   0   \n",
       "100088                      0                        0                   0   \n",
       "100089                      0                        0                   0   \n",
       "100090                      0                        0                   0   \n",
       "100091                      0                        0                   0   \n",
       "100092                      0                        0                   0   \n",
       "100093                      0                        0                   0   \n",
       "100094                      0                        0                   0   \n",
       "100095                      0                        0                   0   \n",
       "100096                      0                        0                   0   \n",
       "100097                      0                        0                   0   \n",
       "100098                      0                        0                   0   \n",
       "100099                      0                        0                   0   \n",
       "\n",
       "        label_Bram Stoker  label_H.Rider Haggard  label_Herman Melville  \\\n",
       "0                       0                      0                      0   \n",
       "1                       0                      0                      0   \n",
       "2                       0                      0                      0   \n",
       "3                       0                      0                      0   \n",
       "4                       0                      0                      0   \n",
       "5                       0                      0                      0   \n",
       "6                       0                      0                      0   \n",
       "7                       0                      0                      0   \n",
       "8                       0                      0                      0   \n",
       "9                       0                      0                      0   \n",
       "10                      0                      0                      0   \n",
       "11                      0                      0                      0   \n",
       "12                      0                      0                      0   \n",
       "13                      0                      0                      0   \n",
       "14                      0                      0                      0   \n",
       "15                      0                      0                      0   \n",
       "16                      0                      0                      0   \n",
       "17                      0                      0                      0   \n",
       "18                      0                      0                      0   \n",
       "19                      0                      0                      0   \n",
       "20                      0                      0                      0   \n",
       "21                      0                      0                      0   \n",
       "22                      0                      0                      0   \n",
       "23                      0                      0                      0   \n",
       "24                      0                      0                      0   \n",
       "25                      0                      0                      0   \n",
       "26                      0                      0                      0   \n",
       "27                      0                      0                      0   \n",
       "28                      0                      0                      0   \n",
       "29                      0                      0                      0   \n",
       "...                   ...                    ...                    ...   \n",
       "100070                  0                      0                      0   \n",
       "100071                  0                      0                      0   \n",
       "100072                  0                      0                      0   \n",
       "100073                  0                      0                      0   \n",
       "100074                  0                      0                      0   \n",
       "100075                  0                      0                      0   \n",
       "100076                  0                      0                      0   \n",
       "100077                  0                      0                      0   \n",
       "100078                  0                      0                      0   \n",
       "100079                  0                      0                      0   \n",
       "100080                  0                      0                      0   \n",
       "100081                  0                      0                      0   \n",
       "100082                  0                      0                      0   \n",
       "100083                  0                      0                      0   \n",
       "100084                  0                      0                      0   \n",
       "100085                  0                      0                      0   \n",
       "100086                  0                      0                      0   \n",
       "100087                  0                      0                      0   \n",
       "100088                  0                      0                      0   \n",
       "100089                  0                      0                      0   \n",
       "100090                  0                      0                      0   \n",
       "100091                  0                      0                      0   \n",
       "100092                  0                      0                      0   \n",
       "100093                  0                      0                      0   \n",
       "100094                  0                      0                      0   \n",
       "100095                  0                      0                      0   \n",
       "100096                  0                      0                      0   \n",
       "100097                  0                      0                      0   \n",
       "100098                  0                      0                      0   \n",
       "100099                  0                      0                      0   \n",
       "\n",
       "        label_Jacob and Wilhelm  label_Mark Twain  label_Mary Shelley  \\\n",
       "0                             0                 0                   0   \n",
       "1                             0                 0                   0   \n",
       "2                             0                 0                   0   \n",
       "3                             0                 0                   0   \n",
       "4                             0                 0                   0   \n",
       "5                             0                 0                   0   \n",
       "6                             0                 0                   0   \n",
       "7                             0                 0                   0   \n",
       "8                             0                 0                   0   \n",
       "9                             0                 0                   0   \n",
       "10                            0                 0                   0   \n",
       "11                            0                 0                   0   \n",
       "12                            0                 0                   0   \n",
       "13                            0                 0                   0   \n",
       "14                            0                 0                   0   \n",
       "15                            0                 0                   0   \n",
       "16                            0                 0                   0   \n",
       "17                            0                 0                   0   \n",
       "18                            0                 0                   0   \n",
       "19                            0                 0                   0   \n",
       "20                            0                 0                   0   \n",
       "21                            0                 0                   0   \n",
       "22                            0                 0                   0   \n",
       "23                            0                 0                   0   \n",
       "24                            0                 0                   0   \n",
       "25                            0                 0                   0   \n",
       "26                            0                 0                   0   \n",
       "27                            0                 0                   0   \n",
       "28                            0                 0                   0   \n",
       "29                            0                 0                   0   \n",
       "...                         ...               ...                 ...   \n",
       "100070                        0                 0                   0   \n",
       "100071                        0                 0                   0   \n",
       "100072                        0                 0                   0   \n",
       "100073                        0                 0                   0   \n",
       "100074                        0                 0                   0   \n",
       "100075                        0                 0                   0   \n",
       "100076                        0                 0                   0   \n",
       "100077                        0                 0                   0   \n",
       "100078                        0                 0                   0   \n",
       "100079                        0                 0                   0   \n",
       "100080                        0                 0                   0   \n",
       "100081                        0                 0                   0   \n",
       "100082                        0                 0                   0   \n",
       "100083                        0                 0                   0   \n",
       "100084                        0                 0                   0   \n",
       "100085                        0                 0                   0   \n",
       "100086                        0                 0                   0   \n",
       "100087                        0                 0                   0   \n",
       "100088                        0                 0                   0   \n",
       "100089                        0                 0                   0   \n",
       "100090                        0                 0                   0   \n",
       "100091                        0                 0                   0   \n",
       "100092                        0                 0                   0   \n",
       "100093                        0                 0                   0   \n",
       "100094                        0                 0                   0   \n",
       "100095                        0                 0                   0   \n",
       "100096                        0                 0                   0   \n",
       "100097                        0                 0                   0   \n",
       "100098                        0                 0                   0   \n",
       "100099                        0                 0                   0   \n",
       "\n",
       "        label_R.M.Ballantyne  \n",
       "0                          0  \n",
       "1                          0  \n",
       "2                          0  \n",
       "3                          0  \n",
       "4                          0  \n",
       "5                          0  \n",
       "6                          0  \n",
       "7                          0  \n",
       "8                          0  \n",
       "9                          0  \n",
       "10                         0  \n",
       "11                         0  \n",
       "12                         0  \n",
       "13                         0  \n",
       "14                         0  \n",
       "15                         0  \n",
       "16                         0  \n",
       "17                         0  \n",
       "18                         0  \n",
       "19                         0  \n",
       "20                         0  \n",
       "21                         0  \n",
       "22                         0  \n",
       "23                         0  \n",
       "24                         0  \n",
       "25                         0  \n",
       "26                         0  \n",
       "27                         0  \n",
       "28                         0  \n",
       "29                         0  \n",
       "...                      ...  \n",
       "100070                     1  \n",
       "100071                     1  \n",
       "100072                     1  \n",
       "100073                     1  \n",
       "100074                     1  \n",
       "100075                     1  \n",
       "100076                     1  \n",
       "100077                     1  \n",
       "100078                     1  \n",
       "100079                     1  \n",
       "100080                     1  \n",
       "100081                     1  \n",
       "100082                     1  \n",
       "100083                     1  \n",
       "100084                     1  \n",
       "100085                     1  \n",
       "100086                     1  \n",
       "100087                     1  \n",
       "100088                     1  \n",
       "100089                     1  \n",
       "100090                     1  \n",
       "100091                     1  \n",
       "100092                     1  \n",
       "100093                     1  \n",
       "100094                     1  \n",
       "100095                     1  \n",
       "100096                     1  \n",
       "100097                     1  \n",
       "100098                     1  \n",
       "100099                     1  \n",
       "\n",
       "[100100 rows x 12 columns]"
      ]
     },
     "execution_count": 305,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "big_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([                        0,                         1,\n",
       "         'label_Alexandre Dumas', 'label_Ambrose Blacklock',\n",
       "            'label_Anthony Hope',       'label_Bram Stoker',\n",
       "         'label_H.Rider Haggard',   'label_Herman Melville',\n",
       "       'label_Jacob and Wilhelm',        'label_Mark Twain',\n",
       "            'label_Mary Shelley',    'label_R.M.Ballantyne'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 306,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "big_frame.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cols=['label_Alexandre Dumas', 'label_Ambrose Blacklock',\n",
    "            'label_Anthony Hope',       'label_Bram Stoker',\n",
    "         'label_H.Rider Haggard',   'label_Herman Melville',\n",
    "       'label_Jacob and Wilhelm',        'label_Mark Twain',\n",
    "            'label_Mary Shelley',    'label_R.M.Ballantyne']\n",
    "y=big_frame[feature_cols]\n",
    "big_frame.to_csv('big_frame.csv', encoding='utf-8')\n",
    "x_trainc=big_frame[0].values.astype('U')\n",
    "max_words=40000\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(x_trainc)\n",
    "sequences = tokenizer.texts_to_sequences(x_trainc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(sequences, y, test_size=0.1, random_state=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 RNN classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_13 (Embedding)     (None, 500, 32)           1440000   \n",
      "_________________________________________________________________\n",
      "lstm_13 (LSTM)               (None, 32)                8320      \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 10)                330       \n",
      "=================================================================\n",
      "Total params: 1,448,650\n",
      "Trainable params: 1,448,650\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/3\n",
      "90090/90090 [==============================] - ETA: 0s - loss: 0.3080 - acc: 0.899 - 570s 6ms/step - loss: 0.3080 - acc: 0.8999\n",
      "Epoch 2/3\n",
      "90090/90090 [==============================] - 568s 6ms/step - loss: 0.3042 - acc: 0.9000\n",
      "Epoch 3/3\n",
      "90090/90090 [==============================] - 571s 6ms/step - loss: 0.3042 - acc: 0.9000\n",
      "Accuracy: 90.00%\n"
     ]
    }
   ],
   "source": [
    "top_words = 45000   \n",
    "sequence_length = 500\n",
    "max_review_length=500\n",
    "\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=sequence_length, padding=\"post\", truncating=\"post\")\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=sequence_length, padding=\"post\", truncating=\"post\")\n",
    "embedding_vecor_length = 32\n",
    "model = Sequential()\n",
    "model.add(Embedding(top_words, embedding_vecor_length, input_length=max_review_length))\n",
    "model.add(LSTM(32))\n",
    "model.add(Dense(10, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "model.fit(x_train, y_train, epochs=3, batch_size=32)\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Countvectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_x=big_frame[0].values.astype('U')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vec = CountVectorizer(stop_words=\"english\", analyzer='word', \n",
    "                            ngram_range=(1, 2), max_df=1.0, min_df=1, max_features=10000)\n",
    "\n",
    "x = count_vec.fit_transform(df_x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x, df_y, test_size=0.2, random_state=4)\n",
    "mnb = MultinomialNB(alpha=0.1)\n",
    "clf=mnb.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = clf.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   precision    recall  f1-score   support\n",
      "\n",
      "  Alexandre Dumas       0.70      0.77      0.73      4513\n",
      "Ambrose Blacklock       0.57      0.70      0.63      1294\n",
      "     Anthony Hope       0.52      0.44      0.48      1023\n",
      "      Bram Stoker       0.59      0.63      0.61      2677\n",
      "  H.Rider Haggard       0.58      0.49      0.53      1489\n",
      "  Herman Melville       0.70      0.72      0.71      3846\n",
      "Jacob and Wilhelm       0.71      0.71      0.71      1935\n",
      "       Mark Twain       0.65      0.49      0.56      1418\n",
      "     Mary Shelley       0.29      0.10      0.15       272\n",
      "   R.M.Ballantyne       0.64      0.59      0.61      1553\n",
      "\n",
      "      avg / total       0.65      0.65      0.65     20020\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 SVM classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3\\envs\\tensorflow1\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   precision    recall  f1-score   support\n",
      "\n",
      "  Alexandre Dumas       0.70      0.78      0.74      4513\n",
      "Ambrose Blacklock       0.60      0.67      0.63      1294\n",
      "     Anthony Hope       0.58      0.38      0.46      1023\n",
      "      Bram Stoker       0.59      0.57      0.58      2677\n",
      "  H.Rider Haggard       0.59      0.40      0.47      1489\n",
      "  Herman Melville       0.66      0.73      0.69      3846\n",
      "Jacob and Wilhelm       0.65      0.71      0.68      1935\n",
      "       Mark Twain       0.65      0.43      0.52      1418\n",
      "     Mary Shelley       0.38      0.92      0.54       272\n",
      "   R.M.Ballantyne       0.66      0.53      0.59      1553\n",
      "\n",
      "      avg / total       0.64      0.64      0.63     20020\n",
      "\n"
     ]
    }
   ],
   "source": [
    "svm_classifier=svm.fit(x_train,y_train)\n",
    "svm_predicted = svm_classifier.predict(x_test)\n",
    "print(classification_report(y_test, svm_predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   precision    recall  f1-score   support\n",
      "\n",
      "  Alexandre Dumas       0.53      0.69      0.60      4513\n",
      "Ambrose Blacklock       0.44      0.51      0.47      1294\n",
      "     Anthony Hope       0.35      0.30      0.33      1023\n",
      "      Bram Stoker       0.42      0.39      0.40      2677\n",
      "  H.Rider Haggard       0.38      0.32      0.35      1489\n",
      "  Herman Melville       0.59      0.51      0.55      3846\n",
      "Jacob and Wilhelm       0.54      0.47      0.50      1935\n",
      "       Mark Twain       0.47      0.33      0.39      1418\n",
      "     Mary Shelley       0.29      0.97      0.44       272\n",
      "   R.M.Ballantyne       0.53      0.39      0.45      1553\n",
      "\n",
      "      avg / total       0.49      0.49      0.48     20020\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Tree_clf = tree.DecisionTreeClassifier()\n",
    "Tree_clf = Tree_clf.fit(x_train, y_train)\n",
    "predicted = Tree_clf.predict(x_test)\n",
    "print(classification_report(y_test, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_12 (Embedding)     (None, 500, 32)           320000    \n",
      "_________________________________________________________________\n",
      "lstm_12 (LSTM)               (None, 32)                8320      \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 10)                330       \n",
      "=================================================================\n",
      "Total params: 328,650\n",
      "Trainable params: 328,650\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/4\n",
      "80080/80080 [==============================] - 481s 6ms/step - loss: 0.3090 - acc: 0.8985\n",
      "Epoch 2/4\n",
      "80080/80080 [==============================] - 473s 6ms/step - loss: 0.3043 - acc: 0.9000\n",
      "Epoch 3/4\n",
      "80080/80080 [==============================] - 472s 6ms/step - loss: 0.3043 - acc: 0.9000\n",
      "Epoch 4/4\n",
      "80080/80080 [==============================] - 469s 6ms/step - loss: 0.3042 - acc: 0.9000\n",
      "Accuracy: 90.00%\n"
     ]
    }
   ],
   "source": [
    "df_x=big_frame[0].values.astype('U')\n",
    "\n",
    "count_vec = CountVectorizer(stop_words=\"english\", analyzer='word', \n",
    "                            ngram_range=(1, 2), max_df=1.0, min_df=1, max_features=5000)\n",
    "\n",
    "x = count_vec.fit_transform(df_x)\n",
    "\n",
    "top_words = 10000   \n",
    "sequence_length = 500\n",
    "max_review_length=500\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x.toarray(), y, test_size=0.2, random_state=4)\n",
    "\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=sequence_length, padding=\"post\", truncating=\"post\")\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=sequence_length, padding=\"post\", truncating=\"post\")\n",
    "embedding_vecor_length = 32\n",
    "model = Sequential()\n",
    "model.add(Embedding(top_words, embedding_vecor_length, input_length=max_review_length))\n",
    "model.add(LSTM(32))\n",
    "model.add(Dense(10, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "model.fit(x_train, y_train, epochs=4, batch_size=32)\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "big_frame['tokenized_column'] = big_frame[0].fillna(\"\").map(word_tokenize)\n",
    "stop = stopwords.words('english')\n",
    "x=big_frame['tokenized_column'].apply(lambda x: [item for item in x if item not in stop])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3\\envs\\tensorflow1\\lib\\site-packages\\ipykernel_launcher.py:2: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.wv.vectors instead).\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "model = gensim.models.Word2Vec(x.tolist(), size=500)\n",
    "w2v = dict(zip(model.wv.index2word, model.wv.syn0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3\\envs\\tensorflow1\\lib\\site-packages\\numpy\\core\\fromnumeric.py:2957: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\tensorflow1\\lib\\site-packages\\numpy\\core\\_methods.py:80: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "X_new = np.array([np.mean([w2v[w] for w in words if w in w2v], axis=0)for words in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [0.08253203, -0.1573896, -0.016781857, 0.11430...\n",
       "1    [0.047489658, -0.25982162, 0.009633851, 0.1504...\n",
       "2    [-0.0013053766, -0.2179021, -0.10905595, 0.126...\n",
       "3    [-0.02052881, -0.31195918, -0.04454572, 0.1752...\n",
       "4    [0.16468629, -0.41880217, 0.13673343, 0.298159...\n",
       "Name: w2v, dtype: object"
      ]
     },
     "execution_count": 319,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "big_frame['w2v']=X_new\n",
    "X=big_frame['w2v']\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-253-b3cba7b4b8b0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdf_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mmnb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMultinomialNB\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mclf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmnb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow1\\lib\\site-packages\\sklearn\\naive_bayes.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    577\u001b[0m             \u001b[0mReturns\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    578\u001b[0m         \"\"\"\n\u001b[1;32m--> 579\u001b[1;33m         \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'csr'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    580\u001b[0m         \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    581\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow1\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[1;34m(X, y, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, warn_on_dtype, estimator)\u001b[0m\n\u001b[0;32m    571\u001b[0m     X = check_array(X, accept_sparse, dtype, order, copy, force_all_finite,\n\u001b[0;32m    572\u001b[0m                     \u001b[0mensure_2d\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mallow_nd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mensure_min_samples\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 573\u001b[1;33m                     ensure_min_features, warn_on_dtype, estimator)\n\u001b[0m\u001b[0;32m    574\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mmulti_output\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    575\u001b[0m         y = check_array(y, 'csr', force_all_finite=True, ensure_2d=False,\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow1\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[0;32m    431\u001b[0m                                       force_all_finite)\n\u001b[0;32m    432\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 433\u001b[1;33m         \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    434\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    435\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: setting an array element with a sequence."
     ]
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(X, df_y, test_size=0.2, random_state=4)\n",
    "mnb = MultinomialNB(alpha=0.1)\n",
    "clf=mnb.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "big_frame['tokenized_column'] = big_frame[0].fillna(\"\").map(word_tokenize)\n",
    "stop = stopwords.words('english')\n",
    "x=big_frame['tokenized_column'].apply(lambda x: [item for item in x if item not in stop])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 309,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import ldamodel\n",
    "\n",
    "num_topics = 8\n",
    "id2word = gensim.corpora.Dictionary(x)\n",
    "corpus = [id2word.doc2bow(text) for text in x]\n",
    "lda = ldamodel.LdaModel(corpus=corpus, id2word=id2word, num_topics=num_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lda_topics(model, num_topics):\n",
    "    word_dict = {};\n",
    "    for i in range(num_topics):\n",
    "        words = model.show_topic(i, topn = 40);\n",
    "        word_dict['Topic # ' + '{:02d}'.format(i+1)] = [i[0] for i in words];\n",
    "    return pd.DataFrame(word_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic # 01</th>\n",
       "      <th>Topic # 02</th>\n",
       "      <th>Topic # 03</th>\n",
       "      <th>Topic # 04</th>\n",
       "      <th>Topic # 05</th>\n",
       "      <th>Topic # 06</th>\n",
       "      <th>Topic # 07</th>\n",
       "      <th>Topic # 08</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>)</td>\n",
       "      <td>?</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>(</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The</td>\n",
       "      <td>„</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>Dick</td>\n",
       "      <td>I</td>\n",
       "      <td>Gutenberg</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>work</td>\n",
       "      <td>The</td>\n",
       "      <td>$</td>\n",
       "      <td>#</td>\n",
       "      <td>“</td>\n",
       "      <td>n't</td>\n",
       "      <td>Project</td>\n",
       "      <td>}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>'ll</td>\n",
       "      <td>works</td>\n",
       "      <td>[</td>\n",
       "      <td>:</td>\n",
       "      <td>'s</td>\n",
       "      <td>said</td>\n",
       "      <td>tm</td>\n",
       "      <td>&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>went</td>\n",
       "      <td>keep</td>\n",
       "      <td>]</td>\n",
       "      <td>said</td>\n",
       "      <td>'</td>\n",
       "      <td>'</td>\n",
       "      <td>Foundation</td>\n",
       "      <td>{</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>little</td>\n",
       "      <td>side</td>\n",
       "      <td>’</td>\n",
       "      <td>I</td>\n",
       "      <td>The</td>\n",
       "      <td>'s</td>\n",
       "      <td>back</td>\n",
       "      <td>new</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>away</td>\n",
       "      <td>stood</td>\n",
       "      <td>«</td>\n",
       "      <td>ye</td>\n",
       "      <td>Indians</td>\n",
       "      <td>:</td>\n",
       "      <td>Literary</td>\n",
       "      <td>To</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>He</td>\n",
       "      <td>one</td>\n",
       "      <td>”</td>\n",
       "      <td>thought</td>\n",
       "      <td>Joe</td>\n",
       "      <td>?</td>\n",
       "      <td>Archive</td>\n",
       "      <td>So</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>found</td>\n",
       "      <td>sat</td>\n",
       "      <td>‘</td>\n",
       "      <td>boy</td>\n",
       "      <td>dog</td>\n",
       "      <td>know</td>\n",
       "      <td>:</td>\n",
       "      <td>looked</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>came</td>\n",
       "      <td>moment</td>\n",
       "      <td>&lt;</td>\n",
       "      <td>come</td>\n",
       "      <td>state</td>\n",
       "      <td>could</td>\n",
       "      <td>http</td>\n",
       "      <td>The</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>small</td>\n",
       "      <td>For</td>\n",
       "      <td>Crusoe</td>\n",
       "      <td>states</td>\n",
       "      <td>donations</td>\n",
       "      <td>would</td>\n",
       "      <td>'s</td>\n",
       "      <td>”</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Then</td>\n",
       "      <td>support</td>\n",
       "      <td>{</td>\n",
       "      <td>see</td>\n",
       "      <td>lay</td>\n",
       "      <td>laws</td>\n",
       "      <td>got</td>\n",
       "      <td>‘</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>last</td>\n",
       "      <td>donations</td>\n",
       "      <td>`</td>\n",
       "      <td>”</td>\n",
       "      <td>going</td>\n",
       "      <td>Tom</td>\n",
       "      <td>Tom</td>\n",
       "      <td>set</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>time</td>\n",
       "      <td>There</td>\n",
       "      <td>:</td>\n",
       "      <td>Tom</td>\n",
       "      <td>?</td>\n",
       "      <td>Joe</td>\n",
       "      <td>night</td>\n",
       "      <td>&lt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>horse</td>\n",
       "      <td>among</td>\n",
       "      <td>&gt;</td>\n",
       "      <td>`</td>\n",
       "      <td>You</td>\n",
       "      <td>see</td>\n",
       "      <td>camp</td>\n",
       "      <td>forth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>round</td>\n",
       "      <td>two</td>\n",
       "      <td>many</td>\n",
       "      <td>help</td>\n",
       "      <td>information</td>\n",
       "      <td>But</td>\n",
       "      <td>agreement</td>\n",
       "      <td>one</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>began</td>\n",
       "      <td>eye</td>\n",
       "      <td>horses</td>\n",
       "      <td>Crusoe</td>\n",
       "      <td>rifle</td>\n",
       "      <td>tell</td>\n",
       "      <td>go</td>\n",
       "      <td>close</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>took</td>\n",
       "      <td>CHAPTER</td>\n",
       "      <td>3</td>\n",
       "      <td>never</td>\n",
       "      <td>compliance</td>\n",
       "      <td>good</td>\n",
       "      <td>seemed</td>\n",
       "      <td>`</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>white</td>\n",
       "      <td>old</td>\n",
       "      <td>almost</td>\n",
       "      <td>cried</td>\n",
       "      <td>contact</td>\n",
       "      <td>shall</td>\n",
       "      <td>electronic</td>\n",
       "      <td>must</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>ground</td>\n",
       "      <td>public</td>\n",
       "      <td>wild</td>\n",
       "      <td>would</td>\n",
       "      <td>tax</td>\n",
       "      <td>'ve</td>\n",
       "      <td>far</td>\n",
       "      <td>chief</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>head</td>\n",
       "      <td>A</td>\n",
       "      <td>seen</td>\n",
       "      <td>looking</td>\n",
       "      <td>OF</td>\n",
       "      <td>Indian</td>\n",
       "      <td>water</td>\n",
       "      <td>also</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>upon</td>\n",
       "      <td>line</td>\n",
       "      <td>along</td>\n",
       "      <td>well</td>\n",
       "      <td>buffalo</td>\n",
       "      <td>get</td>\n",
       "      <td>face</td>\n",
       "      <td>take</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>'s</td>\n",
       "      <td>free</td>\n",
       "      <td>several</td>\n",
       "      <td>What</td>\n",
       "      <td>bear</td>\n",
       "      <td>nothing</td>\n",
       "      <td>Section</td>\n",
       "      <td>old</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>turned</td>\n",
       "      <td>came</td>\n",
       "      <td>boys</td>\n",
       "      <td>body</td>\n",
       "      <td>THE</td>\n",
       "      <td>say</td>\n",
       "      <td>Information</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>look</td>\n",
       "      <td>half</td>\n",
       "      <td>c</td>\n",
       "      <td>cut</td>\n",
       "      <td>told</td>\n",
       "      <td>master</td>\n",
       "      <td>The</td>\n",
       "      <td>run</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>eyes</td>\n",
       "      <td>:</td>\n",
       "      <td>freely</td>\n",
       "      <td>answered</td>\n",
       "      <td>particular</td>\n",
       "      <td>â€œI</td>\n",
       "      <td>home</td>\n",
       "      <td>I</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>one</td>\n",
       "      <td>accepted</td>\n",
       "      <td>fire</td>\n",
       "      <td>time</td>\n",
       "      <td>hold</td>\n",
       "      <td>States</td>\n",
       "      <td>poor</td>\n",
       "      <td>distributed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>They</td>\n",
       "      <td>us</td>\n",
       "      <td>often</td>\n",
       "      <td>ran</td>\n",
       "      <td>efforts</td>\n",
       "      <td>United</td>\n",
       "      <td>number</td>\n",
       "      <td>go</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>long</td>\n",
       "      <td>light</td>\n",
       "      <td>days</td>\n",
       "      <td>anything</td>\n",
       "      <td>4</td>\n",
       "      <td>”</td>\n",
       "      <td>'d</td>\n",
       "      <td>law</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>feet</td>\n",
       "      <td>large</td>\n",
       "      <td>one</td>\n",
       "      <td>one</td>\n",
       "      <td>right</td>\n",
       "      <td>think</td>\n",
       "      <td>distribution</td>\n",
       "      <td>best</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>fell</td>\n",
       "      <td>whole</td>\n",
       "      <td>visit</td>\n",
       "      <td>form</td>\n",
       "      <td>OR</td>\n",
       "      <td>â€˜I</td>\n",
       "      <td>including</td>\n",
       "      <td>solicit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>tree</td>\n",
       "      <td>He</td>\n",
       "      <td>plain</td>\n",
       "      <td>need</td>\n",
       "      <td>savages</td>\n",
       "      <td>ever</td>\n",
       "      <td>volunteers</td>\n",
       "      <td>email</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>She</td>\n",
       "      <td>time</td>\n",
       "      <td>end</td>\n",
       "      <td>equipment</td>\n",
       "      <td>faces</td>\n",
       "      <td>must</td>\n",
       "      <td>towards</td>\n",
       "      <td>limitation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>business</td>\n",
       "      <td>little</td>\n",
       "      <td>'s</td>\n",
       "      <td>permitted</td>\n",
       "      <td>applicable</td>\n",
       "      <td>find</td>\n",
       "      <td>gone</td>\n",
       "      <td>disclaimer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>forward</td>\n",
       "      <td>life</td>\n",
       "      <td>page</td>\n",
       "      <td>minutes</td>\n",
       "      <td>party</td>\n",
       "      <td>enough</td>\n",
       "      <td>day</td>\n",
       "      <td>whale</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>next</td>\n",
       "      <td>Joe</td>\n",
       "      <td>tax</td>\n",
       "      <td>little</td>\n",
       "      <td>Dr.</td>\n",
       "      <td>knew</td>\n",
       "      <td>together</td>\n",
       "      <td>way</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>When</td>\n",
       "      <td>electronic</td>\n",
       "      <td>exempt</td>\n",
       "      <td>rose</td>\n",
       "      <td>Varley</td>\n",
       "      <td>well</td>\n",
       "      <td>come</td>\n",
       "      <td>point</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>house</td>\n",
       "      <td>years</td>\n",
       "      <td>high</td>\n",
       "      <td>But</td>\n",
       "      <td>though</td>\n",
       "      <td>give</td>\n",
       "      <td>If</td>\n",
       "      <td>full</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>cause</td>\n",
       "      <td>sea</td>\n",
       "      <td>long</td>\n",
       "      <td>sun</td>\n",
       "      <td>full</td>\n",
       "      <td>ai</td>\n",
       "      <td>eBooks</td>\n",
       "      <td>terms</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>old</td>\n",
       "      <td>prairie</td>\n",
       "      <td>including</td>\n",
       "      <td>could</td>\n",
       "      <td>WARRANTIES</td>\n",
       "      <td>want</td>\n",
       "      <td>course</td>\n",
       "      <td>thing</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Topic # 01  Topic # 02 Topic # 03 Topic # 04   Topic # 05 Topic # 06  \\\n",
       "0           .           .          )          ?            .          .   \n",
       "1         The           „          .          .         Dick          I   \n",
       "2        work         The          $          #            “        n't   \n",
       "3         'll       works          [          :           's       said   \n",
       "4        went        keep          ]       said            '          '   \n",
       "5      little        side          ’          I          The         's   \n",
       "6        away       stood          «         ye      Indians          :   \n",
       "7          He         one          ”    thought          Joe          ?   \n",
       "8       found         sat          ‘        boy          dog       know   \n",
       "9        came      moment          <       come        state      could   \n",
       "10      small         For     Crusoe     states    donations      would   \n",
       "11       Then     support          {        see          lay       laws   \n",
       "12       last   donations          `          ”        going        Tom   \n",
       "13       time       There          :        Tom            ?        Joe   \n",
       "14      horse       among          >          `          You        see   \n",
       "15      round         two       many       help  information        But   \n",
       "16      began         eye     horses     Crusoe        rifle       tell   \n",
       "17       took     CHAPTER          3      never   compliance       good   \n",
       "18      white         old     almost      cried      contact      shall   \n",
       "19     ground      public       wild      would          tax        've   \n",
       "20       head           A       seen    looking           OF     Indian   \n",
       "21       upon        line      along       well      buffalo        get   \n",
       "22         's        free    several       What         bear    nothing   \n",
       "23     turned        came       boys       body          THE        say   \n",
       "24       look        half          c        cut         told     master   \n",
       "25       eyes           :     freely   answered   particular       â€œI   \n",
       "26        one    accepted       fire       time         hold     States   \n",
       "27       They          us      often        ran      efforts     United   \n",
       "28       long       light       days   anything            4          ”   \n",
       "29       feet       large        one        one        right      think   \n",
       "30       fell       whole      visit       form           OR       â€˜I   \n",
       "31       tree          He      plain       need      savages       ever   \n",
       "32        She        time        end  equipment        faces       must   \n",
       "33   business      little         's  permitted   applicable       find   \n",
       "34    forward        life       page    minutes        party     enough   \n",
       "35       next         Joe        tax     little          Dr.       knew   \n",
       "36       When  electronic     exempt       rose       Varley       well   \n",
       "37      house       years       high        But       though       give   \n",
       "38      cause         sea       long        sun         full         ai   \n",
       "39        old     prairie  including      could   WARRANTIES       want   \n",
       "\n",
       "      Topic # 07   Topic # 08  \n",
       "0              .            (  \n",
       "1      Gutenberg            .  \n",
       "2        Project            }  \n",
       "3             tm            >  \n",
       "4     Foundation            {  \n",
       "5           back          new  \n",
       "6       Literary           To  \n",
       "7        Archive           So  \n",
       "8              :       looked  \n",
       "9           http          The  \n",
       "10            's            ”  \n",
       "11           got            ‘  \n",
       "12           Tom          set  \n",
       "13         night            <  \n",
       "14          camp        forth  \n",
       "15     agreement          one  \n",
       "16            go        close  \n",
       "17        seemed            `  \n",
       "18    electronic         must  \n",
       "19           far        chief  \n",
       "20         water         also  \n",
       "21          face         take  \n",
       "22       Section          old  \n",
       "23   Information            5  \n",
       "24           The          run  \n",
       "25          home            I  \n",
       "26          poor  distributed  \n",
       "27        number           go  \n",
       "28            'd          law  \n",
       "29  distribution         best  \n",
       "30     including      solicit  \n",
       "31    volunteers        email  \n",
       "32       towards   limitation  \n",
       "33          gone   disclaimer  \n",
       "34           day        whale  \n",
       "35      together          way  \n",
       "36          come        point  \n",
       "37            If         full  \n",
       "38        eBooks        terms  \n",
       "39        course        thing  "
      ]
     },
     "execution_count": 313,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_lda_topics(lda, num_topics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
